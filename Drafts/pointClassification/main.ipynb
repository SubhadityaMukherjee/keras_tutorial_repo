{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d51690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import trimesh\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "DATA_DIR = tf.keras.utils.get_file(\n",
    "    \"modelnet.zip\",\n",
    "    \"http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "DATA_DIR = os.path.join(os.path.dirname(DATA_DIR), \"ModelNet10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae42bb5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "mesh = trimesh.load(os.path.join(DATA_DIR, \"chair/train/chair_0001.off\"))\n",
    "mesh.show()\n",
    "points = mesh.sample(2048)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f48412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(num_points=2048):\n",
    "\n",
    "    train_points = []\n",
    "    train_labels = []\n",
    "    test_points = []\n",
    "    test_labels = []\n",
    "    class_map = {}\n",
    "    folders = glob.glob(os.path.join(DATA_DIR, \"[!README]*\"))\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        print(\"processing class: {}\".format(os.path.basename(folder)))\n",
    "        # store folder name with ID so we can retrieve later\n",
    "        class_map[i] = folder.split(\"/\")[-1]\n",
    "        # gather all files\n",
    "        train_files = glob.glob(os.path.join(folder, \"train/*\"))\n",
    "        test_files = glob.glob(os.path.join(folder, \"test/*\"))\n",
    "\n",
    "        for f in train_files:\n",
    "            train_points.append(trimesh.load(f).sample(num_points))\n",
    "            train_labels.append(i)\n",
    "\n",
    "        for f in test_files:\n",
    "            test_points.append(trimesh.load(f).sample(num_points))\n",
    "            test_labels.append(i)\n",
    "\n",
    "    return (\n",
    "        np.array(train_points),\n",
    "        np.array(test_points),\n",
    "        np.array(train_labels),\n",
    "        np.array(test_labels),\n",
    "        class_map,\n",
    "    )\n",
    "\n",
    "\n",
    "def augment(points, label):\n",
    "    # jitter points\n",
    "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n",
    "    # shuffle points\n",
    "    points = tf.random.shuffle(points)\n",
    "    return points, label\n",
    "\n",
    "\n",
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "\n",
    "\n",
    "def tnet(inputs, num_features):\n",
    "\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "\n",
    "    x = conv_bn(inputs, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n",
    "\n",
    "\n",
    "def return_model(NUM_POINTS, NUM_CLASSES):\n",
    "    inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
    "\n",
    "    x = tnet(inputs, 3)\n",
    "    x = conv_bn(x, 32)\n",
    "    x = conv_bn(x, 32)\n",
    "    x = tnet(x, 32)\n",
    "    x = conv_bn(x, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dd7b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "NUM_POINTS = 100\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_points, test_points, train_labels, test_labels, CLASS_MAP = parse_dataset(\n",
    "    NUM_POINTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65898324",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_points, test_labels))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)\n",
    "\n",
    "model = return_model(NUM_POINTS, NUM_CLASSES)\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"./logs/save_at_{epoch}.h5\"),\n",
    "    keras.callbacks.ProgbarLogger(count_mode=\"samples\", stateful_metrics=None),\n",
    "]\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "opt = keras.optimizers.Adam(1e-3)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss_fn,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23443646",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data = test_dataset.take(1)\n",
    "\n",
    "points, labels = list(data)[0]\n",
    "points = points[:8, ...]\n",
    "labels = labels[:8, ...]\n",
    "\n",
    "# run test data through model\n",
    "preds = model.predict(points)\n",
    "preds = tf.math.argmax(preds, -1)\n",
    "\n",
    "points = points.numpy()\n",
    "\n",
    "# plot points with predicted class and label\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "for i in range(8):\n",
    "    ax = fig.add_subplot(2, 4, i + 1, projection=\"3d\")\n",
    "    ax.scatter(points[i, :, 0], points[i, :, 1], points[i, :, 2])\n",
    "    ax.set_title(\n",
    "        \"pred: {:}, label: {:}\".format(\n",
    "            CLASS_MAP[preds[i].numpy()], CLASS_MAP[labels.numpy()[i]]\n",
    "        )\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cb940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
