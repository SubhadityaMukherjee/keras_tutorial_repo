{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb43322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from IPython.display import Image, display\n",
    "from PIL import ImageOps\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd35cdd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"/media/hdd/Datasets/Pets/images/\"\n",
    "target_dir = \"/media/hdd/Datasets/Pets/annotations/trimaps/\"\n",
    "img_size = (160, 160)\n",
    "num_classes = 3\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f063bb8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278ed66",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed139ef",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "display(Image(filename=input_img_paths[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443dcc89",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "img = PIL.ImageOps.autocontrast(load_img(target_img_paths[10]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24586402",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dataset(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            y[j] -= 1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bdcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949fc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.backend.clear_session()\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = 1000\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = Dataset(batch_size, img_size, train_input_img_paths, train_target_img_paths)\n",
    "val_gen = Dataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab7bbe",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "opt = keras.optimizers.RMSprop()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss_fn,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"./logs/save_at_{epoch}.h5\", save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25237e71",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9129b8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "val_gen = Dataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "val_preds = model.predict(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911aa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b875c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for validation image #10\n",
    "i = 10\n",
    "\n",
    "# Display input image\n",
    "display(Image(filename=val_input_img_paths[i]))\n",
    "\n",
    "# Display ground-truth target mask\n",
    "img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
    "display(img)\n",
    "\n",
    "# Display mask predicted by our model\n",
    "display_mask(i)  # Note that the model only sees inputs at 150x150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a79bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
